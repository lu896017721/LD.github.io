<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>LGBM学习记录</title>
    <url>/2022/04/22/LGBM%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h3 id="GBDT-Gradient-Boosting-Decision-Tree">GBDT (Gradient Boosting Decision Tree)</h3>
<p>主要思想是利用弱分类器（决策树）迭代训练以得到最优模型，该模型具有<strong>训练效果好、不易过拟合</strong>等优点。</p>
<p>目前已有的 GBDT 工具基本都是基于预排序的决策树算法(如 Xgboost)。这种构建决策树的算法基本思想是：</p>
<p>首先，对所有特征都按照特征的数值进行预排序。其次，在遍历分割点的时候用O(n)的代价找到一个特征上的最好分割点。最后，找到一个特征的分割点后，将数据分裂成左右子节点。</p>
<p>这样的预排序算法的优点是：能精确地找到分割点。缺点也很明显：首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里需要消耗训练数据两倍的内存。其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</p>
<h3 id="LGBM（LightGBM）">LGBM（LightGBM）</h3>
<p>LightGBM 的动机：常用的机器学习算法，例如神经网络等算法，都可以用 mini-batch 的方式训练，训练数据的大小不会受到内存限制。而 GBDT 在每一次迭代的时候，都需要遍历整个训练数据集多次。如果把整个训练数据装进内存则会限制训练数据集的大小；如果不装进内存，反复遍历训练数据集又会消耗大量时间。尤其面对工业上海量的数据，普通的 GBDT 算法是不能满足其需求的。LightGBM 提出的主要原因就是为了解决 GBDT 在海量数据上遇到的问题，让 GBDT 可以更好地用于工业实践。</p>
<p>LGBM与传统算法相比具有的优点：更快的训练效率；低内存使用；更高的准确率；支持并行化学习；可处理大规模数据；原生支持类别特征，不需要对类别特征再进行0-1编码。</p>
<h4 id="Histogram-算法">Histogram 算法</h4>
<p>直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p>
<p>在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k为超参数），时间复杂度从O(data * feature)优化到O(k * features)。</p>
<p>Histogram 算法由于特征被离散化后，找到的并不是很精确的分割点，所以会对结果产生影响。但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。</p>
<h4 id="Leaf-wise">Leaf-wise</h4>
<p>LGBM抛弃了大多数 GBDT 工具使用的按层生长 (level-wise) 的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise) 算法。Level-wise 过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，而实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，从而带来了很多没必要的开销。</p>
<p>Leaf-wise 则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同 Level-wise 相比，在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度。Leaf-wise 的缺点是可能会长出比较深的决策树，产生过拟合。因此 LightGBM 在 Leaf-wise 之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p>
<h4 id="Gradient-based-One-Side-Sampling-GOSS">Gradient-based One-Side Sampling(GOSS)</h4>
<p>GOSS技术是去掉了很大一部分梯度很小的数据，只使用剩下的去估计信息增益，避免低梯度长尾部分的影响。由于梯度大的数据在计算信息增益的时候更重要，所以GOSS在小很多的数据上仍然可以取得相当准确的估计值。</p>
<h4 id="Exclusive-Feature-Bundling-EFB">Exclusive Feature Bundling(EFB)</h4>
<p>EFB技术是指捆绑互斥的特征（如他们经常同时取值为0），以减少特征的数量。对互斥特征寻找最佳的捆绑方式是一个NP难问题，不过贪婪算法可以取得相当好的近似率，因此可以在不显著影响分裂点选择的准确性的情况下，显著地减少特征数量。</p>
<h3 id="模型创建">模型创建</h3>
<h4 id="LGBM">LGBM</h4>
<h5 id="方式1：使用lgb的风格">方式1：使用lgb的风格</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> LGBM <span class="keyword">as</span> lgb</span><br><span class="line">params_lgb_ = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>, </span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;regression&#x27;</span>, </span><br><span class="line"> </span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>, </span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">50</span>, </span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">6</span>,</span><br><span class="line"> </span><br><span class="line">    <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.8</span>, </span><br><span class="line">    <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.8</span>, </span><br><span class="line">    &#125;</span><br><span class="line">model_lgb = lgb.cv(</span><br><span class="line">    params_lgb_, train_data, num_boost_round=<span class="number">50000</span>, nfold=<span class="number">5</span>, stratified=<span class="literal">False</span>, shuffle=<span class="literal">True</span>, metrics=<span class="string">&#x27;mse&#x27;</span>,</span><br><span class="line">    early_stopping_rounds=<span class="number">50</span>, verbose_eval=<span class="number">50</span>, show_stdv=<span class="literal">True</span>, seed=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h5 id="方式二：使用sklearn风格">方式二：使用sklearn风格</h5>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> LGBM <span class="keyword">as</span> lgb</span><br><span class="line">params_test1 = &#123;<span class="string">&#x27;num_leaves&#x27;</span>:[<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]&#125;</span><br><span class="line">model_lgb = lgb.LGBMRegressor(objective=<span class="string">&#x27;regression&#x27;</span>,</span><br><span class="line">                              max_depth = <span class="number">3</span>,</span><br><span class="line">                              learning_rate=<span class="number">0.1</span>, </span><br><span class="line">                              n_estimators=<span class="number">3938</span>,</span><br><span class="line">                              metric=<span class="string">&#x27;rmse&#x27;</span>, </span><br><span class="line">                              bagging_fraction = <span class="number">0.8</span>,</span><br><span class="line">                              feature_fraction = <span class="number">0.8</span>)</span><br><span class="line">gsearch1 = GridSearchCV(estimator=model_lgb, </span><br><span class="line">                        param_grid=params_test1, </span><br><span class="line">                        scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>, </span><br><span class="line">                        cv=<span class="number">5</span>, </span><br><span class="line">                        verbose=<span class="number">1</span>, </span><br><span class="line">                        n_jobs=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h3 id="LGBMRegressor参数">LGBMRegressor参数</h3>
<p><strong>boosting_type</strong> ：‘gbdt’,‘rf’</p>
<p><strong>n_jobs</strong>  几核cpu</p>
<p><strong>silent</strong> 默认选择True，选择False会输出很多建模中的细节，作用不大还刷屏。</p>
<p><em>注</em>：上面三个参数是要在开始前就确定的</p>
<p><strong>learning_rate</strong>：学习率，初始状态建议选择较大的学习率，设置为0.1.</p>
<p><strong>n_estimators</strong>：树的数量，初始状态适配lr = 0.1</p>
<p><em>注</em>：这两个参数作用于树的数量，不关心树的内部。，这两个参数需要联调</p>
<p><strong>max_depth</strong>：每棵树的最大深度，防止过拟合。初始状态设置3~8。</p>
<p><strong>num_leaves</strong> :每棵树的最多叶子数，因为CART是二叉树，所以叶子数量最大值为2 <strong>depth，所以num_leaves要小于该值才有意义。</strong></p>
<p>min_child_samples**：又称为min_data_in_leaf，指要想建立一个叶子所需要的的最少样本数，增大它可以降低过拟合。**</p>
<p>min_child_weight**：又称为min_sum_hessian_in_leaf，指要想建立一个叶子，该叶子需要提供的最小hessian值。这两个参数都是对新建叶子设置了门槛，可以降低叶子数量，减小过拟合。**</p>
<p>feature_fraction**：每次新建一棵树时，随机使用多少的特征。**</p>
<p><strong>bagging_fraction</strong>：每次进行bagging时，随机使用多少的样本。</p>
<p><strong>bagging_freq</strong>：每建立多少棵树，就进行一次bagging。</p>
<p><strong>reg_alpha</strong>：L1正则化参数</p>
<p><strong>reg_lambda</strong>：L2正则化参数</p>
<p><em>注</em>：上面的参数是调整每棵树的属性</p>
<p><strong>基本调参思路，首先设置lr=0.1确定树的数量，然后调整每颗树的内部参数到最佳。确定树的内部参数后，用该参数，降低lr，反调lr和树的数量。</strong></p>
]]></content>
      <tags>
        <tag>LightGBM</tag>
      </tags>
  </entry>
  <entry>
    <title>Autoformer</title>
    <url>/2022/04/19/Autoformer/</url>
    <content><![CDATA[<h2 id="论文介绍">论文介绍</h2>
<p>论文题目：Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting<br>
<img src="/2022/04/19/Autoformer/1.png" alt="1"></p>
<h2 id="解决的问题">解决的问题</h2>
]]></content>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>时序预测方法总结</title>
    <url>/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="自回归移动平均模型（ARMA）">自回归移动平均模型（ARMA）</h3>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/1.png" alt="image-20220315184103757"></p>
<p>ARMA属于时间序列参数模型。假设$x_t$表示t时刻的时间序列的值，p和q表示时间窗的大小，$\varepsilon_t$表示t时刻的白噪声，$\alpha_1,\dots,\alpha_p$和$\beta_1,\dots,\beta_q$表示权重系数，则：</p>
<p>MA(q)可以表示为：$X_t=\sum_{i=1}^{q}\beta_i\varepsilon_{t-i}+\varepsilon_{t}$，MA模型研究时间序列在t时刻的值与$t-1, t-2, …$ 时刻随机干扰值的相关关系；MA模型主要考察外部影响对变量的影响情况和相应的记忆期限。</p>
<p>AR§可以表示为：$X_t=\sum_{i=1}^{p}\alpha_iX_{t-i}+\varepsilon_t$，AR模型研究第t时刻的序列值受$t-1, t-2, …$时刻的序列值以及当前随机干扰值的影响；AR模型主要考察变量的记忆特征和记忆衰减情况；</p>
<p>ARMA(p,q)可以表示为：$X_t=\sum_{i=1}^{p}\alpha_iX_{t-i}+\sum_{i=1}^{q}\beta_i\varepsilon_{t-i}+\varepsilon_t$，由自回归模型（AR）和移动平均模型（MA模型）为基础“混合”构成。</p>
<h4 id="特点">特点</h4>
<ul>
<li>
<p>ARMA方法作为基于统计的传统时间序列预测方法，其优点是复杂度低、计算速度快。但是针对现实世界复杂的时间序列，传统的单一统计学模型的准确率相对来说会比机器学习差。</p>
</li>
<li>
<p>传统的时间序列预测方法非常依赖参数模型的选择，能 否正确选择参数模型在很大程度上决定了预测结果的准确率。</p>
</li>
<li>
<p>只能适用于单变量时序预测</p>
</li>
</ul>
<h4 id="意义">意义</h4>
<p>传统时间序列预测模型也有其重要的意义：</p>
<ul>
<li>可以作为预测的基准模型，为项目提供一个准确率的基准线，来帮助评估其他模型。</li>
<li>前置清洗作用，时序模型由于其较好的可解释性，可以帮助剔除一些异常值。</li>
<li>作为集成模型中的一块，参与时序集成模型的训练。</li>
</ul>
<h4 id="改进1：ARIMA模型">改进1：ARIMA模型</h4>
<p>ARIMA模型是ARMA模型的推广。当时间序列${X_t}$不满足平稳性时, 我们通常使用<strong>差分</strong>的技巧使序列变得平稳, 然后再应用ARMA模型。使得ARMA模型可以应用于非平稳序列中。但ARIMA模型在长时间序列预测工作表现较差。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/2.png" alt="img"></p>
<h4 id="改进2：VARMA模型">改进2：VARMA模型</h4>
<p>解决ARMA只能适用于单变量时序的局限性。</p>
<p>m维ARMA(p,q)序列，即$VARMA(p,q)$：$X_t=\sum_{j=1}^p A_jX_j+\varepsilon_t-\sum_{j=1}^q B_j\varepsilon_{t-j}$</p>
<p>平稳可逆的VARMA模型具有平稳解，但需要估计出VARMA(p,q)模型的参数$A_1,\dots,A_p,B_1,\dots,B_q$，这是很麻烦的事情。</p>
<p>若q=0，模型退化为m维AR§模型，记为VAR§；</p>
<p>若p=0，模型退化为m维MA(q)模型，记为VMA(q)；</p>
<h4 id="参考">参考</h4>
<p><a href="https://www.jianshu.com/p/6250e60fa28a">多维时间序列——ARMA模型简介、VAR模型</a>、<a href="https://cloud.tencent.com/developer/article/1666552">【时序预测】一文梳理时间序列预测——ARMA模型</a>、<a href="https://www.jianshu.com/p/e52a4b82654e">时间序列模型简介</a></p>
<h3 id="支持向量机（SVM）">支持向量机（SVM）</h3>
<p>支持向量机在回归上的应用之一便是时序预测。为了使用SVR进行非线性回归，使用核函数将输入空间$x(i)$映射到高维特征空间$w(x(i))$。<strong>核函数的使用</strong>是SVR应用的关键。它提供了将非线性数据映射到本质上是线性的“特征”空间的能力。为了使SVR能在时序预测上取得好的效果，也出现了很多改进，如LSSVM、ASVM、$\varepsilon$-DSVM等。</p>
<p>在广泛使用SVR技术的时间序列预测应用中，将SVR视为时间序列预测方法的根本原因是预测问题的非线性方面。传统的基于模型的技术在预测非线性系统生成的时间序列方面通常不如SVR。而当时基于传统人工神经网络（ANN）的多层感知器等模型的性能不一定比SVR好。这可能是由于其固有的局限性，即无法保证网络优化的全局最小值。通过设计，SVR保证了这种全局最小解，并且通常在泛化能力方面具有优越性。随着深度学习的发展，SVR在非线性回归上的优势逐渐减低。</p>
<h4 id="参考-2">参考</h4>
<p>N. I. Sapankevych and R. Sankar, “Time series prediction using support vector machines: A survey,” IEEE Comput. Intell. Mag., vol. 4, no. 2,pp. 24–38, May 2009.</p>
<h3 id="径向基-RBF-函数神经网络">径向基(RBF)函数神经网络</h3>
<p>基函数神经网络是一类特殊的前馈神经网络模型。这类网络的前提是，要逼近的函数可以写成一些基函数的线性展开，并且只用在网络中产生一个隐藏层。基函数神经网络的一个主要优点是：可以使用线性自适应算法（如最小均方（LMS）和递归最小二乘（RLS）算法）来执行学习过程。基函数神经网络的例子包括径向、多项式和小波。这些基函数对输出单元执行非线性数据转换，以产生任意输出函数。</p>
<p>RBF神经网络的基本思想是：用RBF作为隐单元的“基”来构成隐空间，从而将输入矢量直接映射到隐空间，而不需要通过权连接。当RBF的中心点确定以后，这种映射关系也就确定了。而隐含层空间到输出空间的映射是线性的，即网络的输出是隐单元输出的线性加权和。**所以，隐含层的作用是把向量从低维度映射到高维度，这样低维度线性不可分的情况到高维度就可以线性可分了，类似核函数的思想。**这样，网络由输入到输出的映射是非线性的，而网络输出对可调参数而言却又是线性的。网络的权就可由线性方程组直接解出，从而大大加快学习速度并避免局部极小问题。</p>
<p>基函数神经网络用于时序预测，一方面因为它们具有良好的非线性拟合能力，另一方面通过非线性的基函数来实现非线性到线性的映射，从而增加神经网络的性能。</p>
<h4 id="参考-3">参考</h4>
<p><a href="https://www.cnblogs.com/pinking/p/9349695.html">RBF（径向基）神经网络</a></p>
<h3 id="CNN">CNN</h3>
<p>CNN用于时序数据的主要目的是提取时序数据上的特征，在一般情况下，CNN更常用于时序分类问题。</p>
<p>将CNN应用于时间序列预测的想法是学习能代表序列中某些重复模式的过滤器，并使用这些过滤器预测未来值。由于CNN的分层结构，它们可以很好地处理含噪序列，在随后的每一层中丢弃噪声，只提取有意义的模式。</p>
<p>针对时序数据的非线性，提高CNN学习非线性依赖关系能力的一种方法是使用大量的层和过滤器，但往往会遇到学习非线性的能力和过拟合之间的权衡问题。</p>
<h4 id="改进1：TCN（时间卷积网络）">改进1：TCN（时间卷积网络）</h4>
<p>TCN的体系结构与深度前馈神经网络相同，只是每一层的激活值是通过使用前一层的值来计算的。扩张卷积用于选择前一层神经元的哪些值将影响下一层神经元的值。因此，这种扩大的卷积运算捕获了局部和时间信息。</p>
<p>膨胀卷积：膨胀卷积允许卷积时的输入存在间隔采样，采样率受参数d控制。  最下面一层的d=1，表示输入时每个点都采样，中间层d=2，表示输入时每2个点采样一个作为输入。一般来讲，越高的层级使用的d的大小越大。所以，膨胀卷积使得有效窗口的大小随着层数呈指数型增长。这样卷积网络用比较少的层，就可以获得很大的感受野。</p>
<h5 id="优点：">优点：</h5>
<ol>
<li>并行性。可以并行处理数据。</li>
<li>灵活的感受野。TCN的感受野的大小受层数、卷积核大小、扩张系数等决定。可以根据不同的任务不同的特性灵活定制。</li>
<li>稳定的梯度。TCN不太存在梯度消失和爆炸问题。</li>
<li>内存更低。RNN需要将每步的信息都保存下来，从而占据大量的内存，TCN在一层里面卷积核是共享的，内存使用更低。</li>
</ol>
<h5 id="缺点">缺点</h5>
<p>TCN是卷积神经网络的变种，虽然使用扩展卷积可以扩大感受野，但相比于Transformer可以提取任意长度的相关信息的特性还是差了点。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/3.png" alt="image-20220317163822436"></p>
<h4 id="参考-4">参考</h4>
<p><a href="https://arxiv.org/abs/1703.04691">A. Borovykh, S. Bohte, and C. W. Oosterlee, “Conditional time series forecasting with convolutional neural networks,” 2017, arXiv:1703. 04691.</a>、<a href="https://blog.csdn.net/qq_27586341/article/details/90751794/">TCN-时间卷积网络</a></p>
<h3 id="RNN">RNN</h3>
<p>模型的输入是时间序列，其呈现出一个共同的特征，即数据之间存在时间依赖性。传统的神经网络不能考虑到这种依赖关系，RNN正是为了解决这个问题而出现的。</p>
<img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/4.png" alt="image-20220318105605529" width="50%" height="50%">
<h4 id="改进1：LSTM">改进1：LSTM</h4>
<p>标准的基本RNN存在消失梯度问题，即梯度随着层数的增加而减小。实际上，对于具有大量层的深层RNN，梯度实际上变为零，从而阻止了网络的学习。因此，这些网络只具有短期记忆，在处理需要记忆完整序列中包含的所有信息的长序列时，不会获得良好的结果。长短时记忆（LSTM）递归网络的出现是为了解决梯度消失问题。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/5.png" alt="preview"></p>
<h4 id="改进2：GRU">改进2：GRU</h4>
<p>GRU也是长期记忆网络，由于LSTM网络的高计算成本，GRU在作为LSTM的简化版本出现。GRU是在实际应用中对于许多不同的问题都是健壮和有用的。GRU在RNN的基础上使用门控机制使得捕获远程依赖成为可能，同时相对于LSTM有三个门，但GRU通过减少门的数量，使得模型更简单，计算速度更快。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/6.png" alt="image-20220317162500255"></p>
<h4 id="改进3：双向机制（BRNN、BiLSTM）">改进3：双向机制（BRNN、BiLSTM）</h4>
<p>RNN和LSTM都只能依据之前时刻的时序信息来预测下一时刻的输出，但在有些问题中，<strong>当前时刻的输出不仅和之前的状态有关，还可能和未来的状态有关系</strong>。为了获取时间序列在某一时刻前后序列的信息，出现了BRNN等带双向的递归神经网络。主要缺点是在预测之前需要整个数据序列的信息，计算量大。</p>
<h4 id="改进4：DRNN">改进4：DRNN</h4>
<p><strong>DRNN可以增强模型的表达能力，主要是将每个时刻上的循环体重复多次</strong>，每一层循环体中参数是共享的，但不同层之间的参数可以不同。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/7.png" alt="image-20220318110734017"></p>
<h4 id="参考：">参考：</h4>
<p><a href="https://zhuanlan.zhihu.com/p/123211148">史上最详细循环神经网络讲解（RNN/LSTM/GRU）</a>、Torres J F, Hadjout D, Sebaa A, et al. Deep learning for time series forecasting: a survey[J]. Big Data, 2021, 9(1): 3-21.</p>
<h3 id="自编码器">自编码器</h3>
<img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/8.png" alt="image-20220319094312851" width="50%" height="50%">
<p>自编码器(autoencoder) 是神经网络的一种，该网络可以看作由两部分组成：一个编码器函数$h = f(x) $和一个生成重构的解码器$r = g(h)$。自编码器是一种无监督的神经网络模型，它可以学习到输入数据的隐含特征，这称为编码，同时用学习到的新特征可以重构出原始输入数据，称之为解码。从直观上来看，自编码器可以用于特征降维，其相比PCA性能更强。除了进行特征降维，自编码器学习到的新特征可以送入有监督学习模型中，所以自编码器可以起到特征提取器的作用。</p>
<h4 id="改进1：SAE-堆栈自编码器">改进1：SAE(堆栈自编码器)</h4>
<p>即通过堆叠多层的自编码来学习更多的特征，将它用于时序主要还是像CNN那样方便提取特征。</p>
<h4 id="参考-5">参考</h4>
<p><a href="https://zhuanlan.zhihu.com/p/31742653">简单易懂的自动编码器</a></p>
<h3 id="隐马尔可夫模型（HMM）">隐马尔可夫模型（HMM）</h3>
<p>隐马尔可夫模型是关于时序（顺序）的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程。隐藏的马尔可夫链随机生成的状态的序列，称为状态序列；每个状态生成一个观测，而由此产生的观测的随机序列，称为观测序列。序列的每一个位置又可以看作是一个时刻。基本原理：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；</p>
<p>隐马尔可夫模型的基本假设：</p>
<p>1.齐次马尔科夫性假设：即假设隐藏的马尔科夫链在任意时刻t的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关，也与时刻t无关；</p>
<p>2.观测独立性假设：即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态，与其他观测即状态无关。</p>
<h4 id="参考-6">参考</h4>
<p><a href="https://zhuanlan.zhihu.com/p/29938926">隐马尔可夫模型HMM</a></p>
<h3 id="深度置信网络（DBN）">深度置信网络（DBN）</h3>
<p>假设有一个二部图，每一层的节点之间没有链接，一层是可视层，即输入数据层(v)，一层是隐藏层(h)，如果假设所有的节点都是随机二值变量节点（只能取0或者1值），同时假设全概率分布p(v,h)满足Boltzmann 分布，则称这个模型是受限玻尔茨曼机 (RBM)。</p>
<p>深度置信网络：当输入v的时候，通过p(h|v)可以得到隐藏层h，而得到隐藏层h之后，通过p(v|h)又能得到可视层，通过调整参数，使得从隐藏层得到的可视层v1与原来的可视层v如果一样，那么得到的隐藏层就是可视层另外一种表达，因此隐藏层可以作为可视层输入数据的特征。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/9.png" alt="img"></p>
<p>如果把隐藏层的层数增加，同时在靠近可视层的部分使用贝叶斯信念网络（即有向图模型），而在最远离可视层的部分使用RBM，便可以得到DBN。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/10.png" alt="image-20220321105813797"></p>
<h4 id="特点-2">特点</h4>
<p>RBM能够将输入分类到一个特征空间，因此多个RBM层可以在DBN中提取高层特征。学习方式：<strong>逐层贪婪训练</strong>。</p>
<h4 id="参考-7">参考</h4>
<p><a href="https://blog.csdn.net/kellyroslyn/article/details/82668733">DBN(深度置信网络）</a></p>
<h3 id="GAN">GAN</h3>
<p>生成性对抗网络可分为判别网络和生成网络。经过训练的判别网络能够通过学习给定输入输出的条件概率分布来预测给定输入输出。而经过训练的生成网络通过学习输入和输出的联合分布，能够生成与训练样本具有相似分布的样本。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/11.png" alt="image-20220321145501389"></p>
<p>GAN用于时序预测主要因为生成对抗的思想，通过预测网络（如LSTM）与判别网络（如CNN）之间的相互对抗来提升预测网络的预测精度，从而获得较好的预测精度。</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/12.png" alt="image-20220321150651996"></p>
<h4 id="参考-8">参考</h4>
<p>Zhou X, Pan Z, Hu G, et al. Stock market prediction on high-frequency  data using generative adversarial nets[J]. Mathematical Problems in  Engineering, 2018.</p>
<h3 id="Transformers">Transformers</h3>
<p>Transformers对序列数据中的长期依赖关系和交互具有强大的建模能力，因此可以适合于时间序列建模。 利用在输入嵌入中加入的位置编码，对序列信息进行建模。</p>
<p>从网络结构和应用领域的角度看时间序列建模Transformers：</p>
<p><img src="/2022/04/18/%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/13.png" alt="image-20220322162918132"></p>
<h4 id="位置编码">位置编码</h4>
<p>Vanilla Positional Encoding：该编码可以从时间序列中提取一些位置信息，但它们不能充分利用时间序列数据的重要特征。</p>
<p>Learnable Positional Encoding：从时间序列中学习适当的位置编码</p>
<p>Timestamp Encoding：使用数据对应的现实时间戳信息</p>
<h4 id="自注意力机制">自注意力机制</h4>
<p>在神经网络模型处理大量输入信息的过程中，利用注意力机制，可以做到只选择一些关键的的输入信息进行处理，来<strong>提高神经网络的效率</strong>。自注意力机制是注意力机制的变体，其<strong>减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性</strong>。Transformer的核心是自注意力机制。它可以看作是一个全连通层，其权值是根据输入的两两相似度动态生成的。因此，它与全连接层共享相同的最大路径长度，但参数量要少得多，这使得它适合于建模长期依赖关系。</p>
<h4 id="参考-9">参考</h4>
<p><a href="https://zhuanlan.zhihu.com/p/265108616">Attention注意力机制与self-attention自注意力机制</a>、Wen Q, Zhou T, Zhang C, et al. Transformers in Time Series: A Survey[J]. arXiv preprint arXiv:2202.07125, 2022.</p>
]]></content>
  </entry>
</search>
